\newpage
\section{Workflow and Workload Management}
\subsection{The Challenge}
In the past three decades, technological revolution in industry has enabled, and was paralleled by growing complexity in the field of scientific computing, where 
more and more sophisicated methods of data processing and analysis were constantly introduced, oftentimes at a dramatically increased scale. The rate of progress
and constant time pressures to deliver in competetive fields such as HEP led to design and implementation of complete solutions to satisfy the needs of specific  research projects.
This had two consequences:

\begin{itemize}
\item Highly integrated and project-specific design of workflow and workload management (for definitions see~\ref{workflow_workload}).
\item Tight coupling of workflow and workload management to data handling components.
\end{itemize}
This had a substantial impact on reusability of these solutions. In the following, we  bring together a few standard definitions and real life examples to help clarify relationships among distinct elements of this domain and make appropriate recommendations.

\subsection{Description}

\subsubsection{Grid and Cloud Computing}
According to a common definition, Grid Computing is the collection of computer resources from multiple locations to reach a common goal. Oftentimes additional characteristics added to this include decentralized
administration and management and adherence to open standards. It was formulated as a concept in early 1990s, and motivated by the fact that computational tasks handled by large research projects reached
the limits of scalability of most individual computing sites. On the other hand, due to variations in demand, some resources were underutilized at times. There is therefore a benefit in implementing a federation of
computing resources, whereby large spikes in demand can be hadled by federated sites, while ``backfilling'' the available capacity with lower priority tasks submitted by a larger community of users.
Technologies developed in the framework of Grid Computing (such as a few reliable and popular types of \textit{Grid Middleware} became a major enabling factor for many scientific collaborations including
nuclear and high-energy physics.

Cloud Computing is essentially an evolution of the Grid Computing concept, with implied higher degree of computing resources and data storage abstraction, connectivity and transparency of access.
In addition, Cloud Computing is characterized by widespread adoption of \textit{virtualization} - which is also used in the traditional Grid environment but at a smaller scale.
At the time of writing, Cloud prominently figures in the context of commercial services available on the network, whereby computing resources can be ``rented'' for a fee in the form of a Virtual Machine allocated
to the user, or a  number of nodes in the Cloud can be dynamically assigned to perform a neccesary computational task - as a transient, ephemeral resource. Such dynamic, on-demand characteristic
of the Cloud led to it being described as an ``elastic'' resource (cf. ``Amazon Elastic Compute Cloud'' - the EC2).

Regardless of the obvious differentiation of Cloud computing (due to its characteristics as \textit{a utility computing platform} and pervasive reliance on virtualization), many of its fundamental concepts and challenges
are common with its predesessor, Grid Computing. In the following, we won't distinguish between the two unless  necessary.


%
% There is also the need for seemless lightweight unit testing, regression testing and module development on small local systems outside of large grid or cloud environments.\\
% -mxp- Brian - good point but it obfuscates the simple statement re: cloud. I just wanted this t obe minimalistic - we will definitely put this statement in the document, just in a different loication.

\subsubsection{From the Grid to Workload Management}
\label{from_grid_to_workload}
Utilization of the Grid sites via appropriate middelware does establish a degree of resource federation, but it leaves it up to the user to manage data movement and job submission to multiple sites,
track job status, handle failures and error conditions, aggregate bookkeeping information and perform many other tasks. In absense of automation, this does not scale very well and limits the efficacy
of the overall system.

It was therefore inevitable that soon after the advent of reliable Grid middleware multiple physics experiments and other projects started developing and deploying \textit{Workload Management Systems} (WMS).
 According to one definition,
\textit{``the purpose of the Workload Manager Service (WMS) is to accept requests for job submission and management coming from its clients and take the appropriate actions to satisfy them''}~\cite{egee_user_guide}.
Thus, one of the principal functions of a Workload Management System can be described as ``brokerage'', in the sense that it matches resource requests to the actual distributed resources
on multiple sites. This matching process can include a variety of factors such as access rules for users and groups, priorities set in the system, or even data locality - which is in fact an important and interesting part of this process~\cite{panda_chep10}.

In practice, despite differing approaches and features, most existing WMS appear to share certain primary objectives, such as:
\begin{itemize}
\item{Insulation of the user from the complex and potentially heterogeneous environment of the Grid, and shielding the user from common failure modes of the Grid infrastructure}
\item{Rationalization, bookkeeping and automation of software provisioning - for example, distribution of binaries and configuration files to multiple sites}
\item{Facilitation of basic monitoring functions, e.g. providing efficient ways to determine the status of jobs being submitted and executed}
\item{Prioritization and load balancing across the computing sites}
\item{Implementation of interfaces to external data management systems, or actual data movement and monitoring functionality built into certain components of the WMS}
\end{itemize}


\subsubsection{Workflow vs Workload}
\label{workflow_workload}
A scientific workflow system is a specialized case of a \textit{workflow management system}. In it, computations and/or transformations and exchange of data are performed according to a defined set of rules
in order to achieve an overall goal ~\cite{grid_workflow_taxonomy},~\cite{grid_workflow_fit}. In the context of this document, this process involves execution on distributed resources. Since the process is
typically largely (or completely) automated, it is often described as ``orchestration'' of execution of multiple interdependent tasks. Workflow systems are sometimes described using the concepts of a \textit{control flow},
which refers to the logic of execution, and \textit{data flow}, which concerns itself with the logic and rules of transmitting data.

A simple and rather typical example of a workflow is often found in Monte Carlo studies performed in High Energy Physics and related fields, where there is a chain of processing steps similar to the pattern below:
\\
\\
\textit{Event  Generation $\Longrightarrow$ Simulation $\Longrightarrow$ Digitization $\Longrightarrow$ Reconstruction}
\\
\\
Patterns like this one may also include optional additional steps (implied or made explicit in the logic of the workflow) such as merging of units of data (e.g.files) for more efficient storage and transmission.

Conceptually, this level of abstraction of the workflow does not  involve issues of resource provisioning and utilization, monitoring, optimization, recovery from errors, as well as plethora of other items essential
for efficient execution of workflows in the distributed environment. These tasks are handled in the context of \textit{Workload Management} which we very briefly described in~\ref{from_grid_to_workload}.

In summary, we make a distinction between the \textit{Workflow Management} domain which concerns itself with controlling the scientific workflow, and \textit{Workload Management} which
is a domain of resource provisioning, allocation, execution control and monitoring of execution etc. The former is a level of abstraction above Workload Management, whereas the latter is in
turn a layer of abstractions above the distributed execution environment such as the Grid or Cloud.

% As we already noted, 
% Historically, in many cases the Workflow Management layer was not designed together with the underlying Workload Management system, and was (\textbf{FIXME})

\subsubsection{The Role and Engineering of Data Management}
In most cases of interest to us, data management plays crucial role in reaching the scientific goals. It is covered separately (see Section~\ref{data}).
As noted above, it represents the \textit{data flow} component of the overall workflow management and therefore needs to be addressed here as well.


\subsubsection{Late Binding}
As we mentioned in ~\ref{from_grid_to_workload}, one of the primary functions of a WMS is to insulate the user from the heterogeneous and sometimes complex
Grid environment and certain failure modes inherent in it (e.g. misconfigured cites, transient connectivity problems, ``flaky'' worker nodes etc).
There is a proven solution to these issues, which involved the so called \textit{late binding} approach  to the deployment of computational payload.
According to this concept, it is not the actual ``payload job'' that is initially dispatched to a  Worker Node residing in a
remote data center, but an intelligent ``wrapper'', often termed a ``pilot job'', which first validates the resource, its configuration and some details of the environment
(for example, outbound network connectivity may be tested). In this context, ``binding'' means matching process whereby the payload job (such as a production job or
a user analysis job) which is submitted to the WMS and is awaiting execution is assigned to a live pilot which has already perfomed validation and configuration of
the execution environment. The part of the overall resource pool that exhibit problems prior to job dispatch can be identified and flagged at both site and worker node
level. This eliminates a very large fraction of potential failures that the user would otherwise have to deal with and account for.
% Thus, for the user (or for an automated client performing job submission) x

An added benefit of this approach is also diagnostic and logging capability that maybe included
in the pilot. This is extremely important for troubleshooting and monitoring, which we shall discuss later.


\subsection{Examples}
\label{wms_examples}
\subsubsection{HTCondor and glideinWMS}
HTCondor~\cite{htcondor} is perhaps the best known and  very important set of Grid and High-Throughput Computing (HTC) tools. It provides Workload Management System functionality,


\subsubsection{LHC}
There are a variety of Workflow and Workload Management systems deployed in recent past and at present. Our goal here is not to compile an exhaustive list of such systems but rather pick a (very) few representative examples, which have been deployed and utilized \textit{at scale}.
Let's start with the LHC experiments:

\begin{center}
  \begin{tabular}{ c | c | c | c }
    \hline
    \textbf{Primary User} & \textbf{Workload Mgt} & \textbf{Workflow Mgt} & \textbf{Data Mgt}\\ \hline \hline
    ATLAS & PanDA & ProdSys2 & Rucio\\ \hline
    CMS  & GlideinWMS & Crab3 & PhEDEx\\ \hline
    LHCb  & DIRAC & DIRAC Production Management & DIRAC DMS\\ \hline
    Alice  & gLite WMS & AliEn & AliEn\\ 
%    Small Exp & BOINC & CONDOR & mySQL/Postgres \\ \hline
% -mxp- the preamble to the table mentions deployment at scale. Also mySQL in itself does not fill the role of any component mentioned. To better describe this, we add more text below!
    \hline
  \end{tabular}
\end{center}

\subsubsection{@HOME}
TBD - BOINC etc.

\subsubsection{What works, what doesn't}
\subsubsection{Examples}
\subsection{Opportunity for improvement}


