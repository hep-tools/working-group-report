\section{Event Processing Software Frameworks}
\textit{(authors: Brett, ??)}

\subsection{Description}

A software framework abstracts common functionality expected in some
domain.  It provides some generic implementation of a full system in
an abstract way that lets application-specific functionality to be
added through a modular implementation of framework interfaces.

Whereas toolkit libraries encourages different functionality through
developing different applications (toolkits provide tools to design for a specific task), a framework provides the overall
processing control function with typically one main entry point and through module implementation tasks are completed within the processing flow (the framework provides the infrastructure need for a tool to function).
Different functionality is obtained by configuring the framework to
use different module implementations.

In the context of HEP software, the terms ``event'' and ``module" are an overloaded and poorly defined terms.
%The term ``module'' is probably second.  Both are used here.  
In the context of software frameworks, an ``event'' is a unit of data whose scope is dependent
on the ``module'' of code which is processing.  In the context of a
code module that generates initial kinematics, an event is the
information about the interaction.  In a module that simulates the
passage of particles through a detector, an event may contain all
energy depositions in active volumes.  In a detector electronics
simulation, it may contain all signals collected from these active
volumes.  In a trigger simulation module, it would be all readouts of
these signals above some threshold or other criteria.  At this point,
data from real detectors gain symmetry with simulation.  Beyond this
data reduction, calibration, reconstruction and other analysis modules
each have a unique concept of the ``event'' of data they operate on.
Depending on the nature of the physics, detector, and follow on
analysis, every module may not preserve the multiplicity of data.  For
example, a single interaction may produce multiple triggers, or none.

With that description, an event processing software framework is
largely responsible to marshal data through a series (in general a
graph) of such code modules which then mutate the data.  To support
these modules the framework provides access to external services such
as those that give access to the data itself, handle file I/O, access
to descriptions of the detectors, provide for visualization or
statistical summaries, and databases of conditions for applying
calibrations. How a framework marshall and associate data together as an event is largely varied across different HEP experiments and may be unique for a given data collection methodology (beam gate, online trigger, raw timing, etc). 

%This paragraph is a very good comment, but it isn't really a description of general frameworks. It is a description of a well implemented framework. I think that we should move this to recommendations, etc.

The defining nature of these frameworks are to aggregate and organize
code modules provided by a disparate set of developers.  As such,
quality frameworks provide means to manage this complexity.  The
modular design itself is one.

\subsection{Gaudi}

The Gaudi event processing framework provides a comprehensive set of
features and is extensible enough that it is suitable for a wide
variety of experiments.  It was conceived by LHCb and adopted by ATLAS
and these two experiments still drive its development.  It has been
adopted by a diverse set of experiments including HARP, Fermi/GLAST,
MINER$\nu$A, Daya Bay and others.  The experience of Daya Bay is
illuminating for both Gaudi specifically and of more general issues of
this report.

First, this adoption was greatly helped by the support from the LHCb
and ATLAS Gaudi developers.  Although not strictly their
responsibility, they found the time to offer help and support to other
experiments.  Without this, the success of the adoption is uncertain
and at best would have taken much more effort.  Daya Bay recognized
the need and importance of such support and, partly selfishly, formed
a mailing list\cite{gauditalk} and got Gaudi developers from many
experiments involved.  It became a forum that more efficiently spread
beneficial information from the main developers as well as offloaded
some support to new experts from the other experiments to help
themselves.

There were, however areas that would improve the adoption of Gaudi.
The primary one would be direct guides on how to actually adopt it.
This is something that must come from the community and likely in
conjunction with some future adoption.  Documentation on Gaudi itself
was also a problem particularly for Daya Bay where many of the basic
underlying framework concepts where new.  Older design documents and
some experiment-specific ones were available but not always accurate
nor focused on what was needed.  Over time Daya Bay produced it's own
Daya Bay-specific documentation which unfortunately perpetuates this
problem.

Other aspects were beneficial to adoption.  The Gaudi build system
based on CMT (is there a reference for CMT?) was cross platform, open and easy to port.  It had layers
of functionality (package build system, release build system, support
for experiment packages and ``external'' ones) but it did not require
all-or-nothing adoption.  This allowed for some staged approach that
allowed Daya Bay to get started using the framework more quickly.

The importance of having all Gaudi source code open and available can
not be diminished.  Also important was Gaudi developers inclusion of
the growing community in the release process.

While Gaudi's CMT-based package and release build system ultimately
proved very useful, it hampered initial adoption as it is not commonly
used and required understanding.  It is understood that there is a
movement to provide a CMake based build system which may alleviate
this hurdle.

Finally, although Gaudi is full featured and flexible it does not come
with all framework level functionality and, in its core, misses some
important extensions.  In particular, Daya Bay adopted three Gaudi
extensions from LHCb's code base.  These are actually very general
purpose but due to historical reasons were not provided separately.
These were GaudiObjDesc (data model definition), GiGa (Geant4
interface) and DetDesc (detector description).  Some extensions
developed by other experiments were rejected and in-house
implementations were developed.  In particular the extension that
provided for file I/O was considered too much effort to adopt.   

One aspect of the default Gaudi implementation that had to be modified for use by Daya
Bay was the event processing model.  Unlike collider
experiments, Daya Bay necessarily had to deal with a non-sequential,
linear event stream.  Multiple detectors at multiple sites produced
data in time order but not synchronously.  Simulation and processing
did not preserve the same ``event'' multiplicity.  Multiple sources of
events (many independent backgrounds in addition to signal) must be
properly mixed.  Finally, of the strongest background rejection
criteria to select inverse beta decay from anti-neutrino interactions
is to require a coincidence in time between a prompt positron
interaction in the scintilator followed by delayed activity from the
absorption of a neutron.  The flexibility of Gaudi allowed Daya Bay to
extend its very event processing model to add the support necessary
for these features.

\subsection{CMSSW and art}

In 2006, the CMS Experiment developed their current software framework, CMSSW \cite{cmssw}, as a replacement to the previous ORCA framework. The framework was built around two guiding principles: the modularity of software development and that exchange of information between modules can only take place through data products. Since implementing the CMSSW, the complexity of the CMS reconstruction software was greatly reduced compared with ORCA and the modularity lowered the barrier to entry for beginning software developers.

CMS Software Framework is designed around four basic elements: the framework, the event data model, software modules written by physicists, and the services needed by those modules\cite{cmssw_web}. The framework is intended to be a lightweight executable (cmsRun) that loads modules dynamically at run time. The configuration file for cmsRun defines the modules that are part of the processing and thus the loading of shared object libraries containing definitions of the modules. It also defines the configuration of modules parameters, the order of modules, filters, the data to be processed, and the output of each path defined by filters. The event data model (EDM) has several important properties: events are trigger based, the EDM contains only C++ object containers for all raw data and reconstruction objects, and it is directly browsable within ROOT. Another important feature of the EDM over the ORCA data format was the requirement that all information about an event is contained within a single file. The framework was also constructed such that the EDM would contain all of the provenance information for all reconstructed objects. Therefore it would be possible to regenerate and reproduce any processing output from the raw data given file produced from CMSSW.

The art framework is event processing framework that is an evolution of the CMSSW framework. In 2010, the Fermilab Scientific Computing Division undertook the development of an experiment-agnostic framework for use by smaller experiments that lacked the personpower to develop a new framework. Working from the general CMSSW framework, most of the design elements were maintained: lightweight framework based on modulular development, event data model, and services required for modules. The output file is ROOT browsable and maintains the strict provenance requirements of CMSSW. For Intensity and Cosmic Frontier experiments, the strict definition of an event being trigger based isn't appropriate and so this structuring was removed and each instance of art allows the experiment to define the event period of interest as required. art is currently being used by the Muon g-2, $\mu2e$, NOvA, $\mu BooNE$, and LBNE/F experiments.

CMSSW did initially have some limitations when implemented, the most significant being the use of non-interpreted, run-time configuration files defined by the FHiCL language. The significance of this being that configuration parameters could not be evaluated dynamically and were required to be explicitly set in the input file. This limitation meant it was impossible to include any scripting within the configuration file. This limitation was recognized by the CMS Collaboration and they quickly made the choice to instead transition to Python based configuration files. Due to the requirement within CMSSW for strict inclusion of provenance information in the EDM, the dynamic evaluation of configuration files was not considered a limitation to reproduction from raw data. The art framework has not made the transition and continues to use FHiCL language configuration files, and, while acceptable to experiments at the time of adoption, some consider this a serious limitation.

One of the challenges faced by the art framework that has been the portability of the framework to platforms other than Scientific Linux Fermilab or Cern. The utilization of the Fermilab UPS and cetbuildtools products within the build and release system that was integrated into the art suite resulted in reliance upon those products that is difficult to remove and therefore port to other platforms (OS X, Ubuntu, etc). While this is not an inherent problem of the software framework, and is currently being addressed by both Fermilab SCD and collaborative experiments, it serves as a significant design lesson when moving forward with art or designing other frameworks in the future.

\subsection{IceTray}

\subsection{What works, what doesn't}

	ROOT browsable data files\\
	module implementation of reconstruction producers through standardized entry points and hooks\\
	Event Data Model including strict provenance information requirements\\
	flexible I/O based upon metadata\\
	portability of both code and build mechanism\\
	
	Particularly in C++ based frameworks,
these modules are implemented by inheriting from a framework-provided
base class. In order to avoid hard-linking which modules to use at
compile time they are best put into shared libraries which can be
loaded at run time as driven by a user configuration layer provided by
the framework.  Even with this, compile-time dependencies can balloon
build times and frameworks should best implement the base classes with
pure-virtual interface classes such that changes to one part of the
code base do not require a full rebuild/relink.

\subsection{Examples}
\subsection{Opportunity for improvement}


\subsubsection{Editing notes}


\textit{This is not a real section and will be deleted}

\begin{itemize}
\item types of software frameworks
\item focus on event data processing FW:
  \begin{itemize}
  \item provides a ``main'' program
  \item determines execution model
  \item provides hooks/entries for user/programmers to supply
    \begin{itemize}
    \item start/end of run/subrun/file
    \item once-per-event entry
    \end{itemize}
  \item end-user job configuration and scripting
  \item provides functionality to manage compile- and run-time complexity, (Interface classes, shared library autoload/plugin)
  \item provides software development and release management support
  \end{itemize}
\end{itemize}
Some frameworks
\begin{itemize}
\item Gaudi
\item RAT
\item IceTray
\item CMSSW
\item Art
\end{itemize}
Comparisons:
\begin{itemize}
\item for Daya Bay \url{https://wiki.bnl.gov/dusel/index.php/Software_Framework_Comparison}
\end{itemize}
