\section{Data Management}
\label{data}

\fixme{(authors: Anders, Brian, Maxim, Mike, Simon)}

\subsection{Definition}
In general, \textbf{Data Management} is any \textit{content neutral} interaction with the data, i.e. it is the \textit{data flow}
component of the larger domain of  \textbf{Worklflow Management} (see \ref{workflow_workload}). It addresses issues of data storage
and archival, mechanisms of data access and distribution, and  curation -- over the full life cycle of the data. In order to remain within
the scope of this document and keep  proper focus on topics most relevant for the ``Working Group on Libraries and Tools'', we will
concentrate our attention on issues related to data distribution, metadata and catalogs, and won't cover issues of mass storage
in much detail. Likewise, for the most part network-specific issues fall outside of our purview.

% It includes technical solutions, procedures and policies and deals with the full life cycle of the data. 


\subsection{Moving Data}
\subsubsection{Modes of Data Transfer and Access}
In any distributed environment (and most HEP and IF experiments are prime examples of that) the data is typically stored at multiple locations,
for a variety of reasons, and over its lifetime undergoes a series of transmissions over networks, replications and/or deletions, with attendant bookkeeping
in appropriate catalogs. Data networks utilized in research can span \textit{hundreds} of Grid sites across multiple continents.

In HEP, we observe a few different and distinct modes of moving and accessing data. Consider the following:

\begin{description}
\item[Bulk Transfer] In this conceptually simple case, data transport from point A to point B is automated and augmented
with redundancy and verification mechanism so as to minimize chances of data loss. Examples of this can be found in SPADE
(data transfer system used in Daya Bay) and components of SAM at FNAL.

\item[Managed Replication] In many cases the strategy consists on creating replicas of certain segments of the data (datasets, blocks, etc)
on participating Grid sites. Such distribution is done according to a previously developed policy which may be based on storage capacities of 
the sites, specific processing plans, resource quota and any number of other factors. Good examples of this type of systems are found in
ATLAS (\textit{Rucio)} and CMS (\textit{PhEDEx}), among other experiments~\cite{rucio_chep13,phedex_chep09}.

\item[``Data in the Grid'' (or Cloud)] In addition to accessing the data which is local to the processing element, such as a Workder Node
on the Grid, it is possible to access data that it physically remote, provided there exists enough bandwidth between the storage
facility or device, and the processing element. This can be as simple as using \textit{http} to pull data from a remote location before
executing the payload job, or as advanced as utilizing \textit{xrootd}~\cite{xrootd,xrootd_web} over WAN to federate storage resources and locate and
deliver files transparently in a ``just-in-time'' manner.

\end{description}


 Choosing the right approaches and technologies in this case is obviously a two-tiered process -- first, one needs to identify the most
 relevant use cases and match them to categories such as outlined above (e.g. replication vs network data on demand). Second, within
 the chosen scenario, proper solutions must be identified (and hopefully reused rather than reimplemented).
   
% The bulk raw data need to be transfered from where they are taken to the main storage site (which sometimes is also the 
%main processing site). Sometimes this may be on the same location (LHC, Tier 0 at CERN), other times the data may be transfered over large 
%distances and even between continents (Daya Bay). These tools can be file transfer utilities or additional tools layered on top of such utilities. 
%Some of these tools are:  
%\begin{itemize}
%\item bbcp
%\item gridFTP
%\item SPADE 
%\end{itemize}





\subsubsection{Data Network Topologies}
\fixme{Build a logic bridge here. Monarc etc.)}

\begin{description}
\item[Tiers] For smaller experiments there will only be one processing site and it is co-located at the main storage site. For larger experiments 
a tiered system is prefered as no single site has enough capacity. Tier 0 will be the main site, usually storing all the raw data. Tier 1 sites will 
usually be large sites that can contain reconstructed data. Tier 2 sites will then be sites with smaller capacity, but still enough to contain 
analysis data (or at least a subset of this). Note that processing can be decoupled from the (storage) sites if it uses grid computing. Also note 
that data sets may be moved around by a WMS like PanDA, for example, to where the processing is.  

\item[File Access Protocols] Once files have been transfered and processed, they will eventually be accessed by the users (with the help of 
Data Catalogs, see below). File access protocols are not to be confused with file transfer tools which is moving a file from A to B. 
A well known example, XRootD, is at the lowest level a protocol for opening and reading (writing) a file across the network. Multiple XRootDs can 
be put together, and via the simple functionality to redirect and reconnect, can be used to shield the client from the details of the storage 
behind the scenes (SSD, fast/slow disks, tape). One interesting development is to also shield the geographical location of the file from the user i.e. 
Federated Storage.

\item[Federated Storage] XRootD redirectors can be put into a system of mutiple layers. A user request for a file goes first to the top 
redirector and can then cascade down until it reaches a machines that has the file. This way one can build up a federated storage model i.e. 
'World federation' containing 'US' and 'Europe' and 'XXX' federations, and where 'US' again can have multiple layers. A user requesting a file 
will in this model not care where the file is geographically located i.e. all the data will be available all the time everywhere. Underlying this 
model is the fact that today's networks have enough bandwidth to shuffle all this data around. 
Note that the original LHC computing model was the opposite of this i.e. processing was moved to where the data were stored.
\end{description}


\subsection{Metadata}
To be able to manage the data it has to be described. Metadata is therefore a necessary part of data management. The 
list of possible metadata is long. Some key ones are:

\begin{itemize}
\item Data Provenance i.e. when and where it was taken, which raw data were used if the data are processed etc.
\item Data processing description i.e. processing status including calibrations, code versions etc.
\item Data location i.e. where the files are located (especially since data may be moved around). Note that the meaning of this changes in a 
Federated Storage Model. 
\item Data description i.e. analysis summary information.
\end{itemize}



\subsection{Data catalogs And Logical Data Sets}
A data catalog combines a file catalog i.e. information about where the data files are stored, with additional metadata.
This enables the construction of logical (virtual) data sets like 'WIMPcandidatesLoose' and makes it possible for users to 
make a selection of a subset of the available data.

\begin{description}
\item[Fermi Data Catalog] One way to do this is to register all the metadata when you register the file. A slightly
different approach was chosen by the Fermi Space Telescope Data Catalog. In addition to the initial metadata, it had a
data crawler that would go through all registered files and extract metadata like number of events etc. The advantage is
that the set of metadata then can be easily expanded after the fact - you just let loose the crawler with the list
of new quantities to extract which then is just added to the existing list of metadata). Obviously this only works for 
metadata included in the file and file headers.  Note that since the Fermi 
Data Catalog is independent of any workflow management system, any data processing metadata will have to be explicitly added. 


\item[SAM] [more detailed description - SAM is more tied into workflow management than the Fermi Data catalog]

\item[ATLAS] ....

\item[CMS] ....

\end{description}



\subsection{Data distribution}

\subsubsection{Data Skimmers}
A Data Catalog can be combined with a workflow engine to produce data skims. These can be either user skims or collaboration 
wide skims. 

\begin{description}
\item[Fermi data skimmer] Even though the Fermi Data Catalog is independent of any workflow management system, it 
can be chained together with for example the Fermi workflow engine ('pipeline') to easily produce user defined skims 
i.e. the user selects cuts and what logical data sets to use and the relevant file locations are automatically 
forwarded to the processing pipeline that produce the requested output files and make them available for download 
by the user. The drastic reduction in data volume makes this an attractive solution for many experiments since the skimmed files 
may be stored and analyzed locally, in some cases even on a laptop.
\end{description}

\subsubsection{Authentication/Authorization} 


\subsubsection{Hot Datasets}
All data are not created equal. There will inevitably be hot data sets (Higgs skims for example) accessed often and by 
many. There will also be colder data sets rarely accessed at all. The temperature of the different datasets maps  
naturally onto different storage technology. For example, in a three-tiered system the hot datasets are stored on SSD, 
warm datasets are stored on fast disks and cold datasets are stored on slow disks or tape. XRootD makes the 
management of such a system easier as it can be configured to automatically retrieve datasets from tape in case they 
don't exist on disk. Note that such a system is dynamic i.e. today's hot dataset is tomorrow's cold one so datasets will 
need to be automatically migrated from one storage layer to the following one based on access patterns. [add some details 
about the ATLAS/CMS systems for caching and purging] 



\subsection{What Works, What Doesn't}
[Needs to be expanded] One danger with DM systems is that they become monolithic - especially over time - as more 
and more functionality is added and there is a desire to make the overall system easier to use (automating more 
and more functions). Maybe somebody has some experience with how LHC experiments have tried to make things modular?


\subsection{Common Elements vs Experiment Specific Ones}



\subsubsection{Editing notes}

\fixme{This is not a real section and will be deleted}

\begin{itemize}
\item Define data management and it's major elements (one suggestion is below)
\item reference that it is related to workflow management
\item List ways that DM can tend to be particularly parochial and how in some cases it must be specifically tailored to the computing facility and/or experiment.
\item List what elements are general.
\item Take care not to overlap with the systems group, but to point out where there are areas of overlap.
\end{itemize}
Possible partitioning of DM into smaller parts:
\begin{description}
\item[Distribution] issues:
  \begin{itemize}
  \item authentication/authorization
  \item caching / purging
  \item side hardware and software requirements
  \item on-demand vs. scheduled
  \end{itemize}
\item[Metadata] issues:
  \begin{itemize}
  \item What job produced what file and with what input parameters/data
  \item Where is my file? Data catalogs ...
  \item Fileset definitions
  \item File popularity to drive cache purges
  \item Important analysis summary info
  \item Locating files/events/objects
  \item Everything as metadata
  \item Provenance tracking
  \item Namespace issues
  \end{itemize}
\end{description}



\subsection{Description}
\subsection{What works, what doesn't}
\subsection{Examples}
\subsection{Opportunity for improvement}
