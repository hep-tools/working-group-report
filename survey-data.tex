\section{Data Management}
\label{data}

\fixme{(authors: Anders, Brian, Maxim, Mike, Simon)}

\subsection{Definition}
In general, \textbf{Data Management} is any \textit{content neutral} interaction with the data, i.e. it is the \textit{data flow}
component of the larger domain of  \textbf{Workflow Management} (see \ref{workflow_workload}). It addresses issues of data storage
and archival, mechanisms of data access and distribution, and  curation -- over the full life cycle of the data. In order to remain within
the scope of this document we will
concentrate on issues related to data distribution, metadata and catalogs, and won't cover issues of mass storage
in much detail (which will be covered by the \textit{Systems} working group). Likewise, for the most part network-specific issues fall outside of our purview.

% It includes technical solutions, procedures and policies and deals with the full life cycle of the data. 


\subsection{Moving Data}
\subsubsection{Modes of Data Transfer and Access}
In any distributed environment (and most HEP and IF experiments are prime examples of that) the data are typically stored at multiple locations,
for a variety of reasons, and over its lifetime undergoes a series of transmissions over networks, replications and/or deletions, with attendant bookkeeping
in appropriate catalogs. Data networks utilized in research can span \textit{hundreds} of Grid sites across multiple continents.

In HEP, we observe a few different and distinct modes of moving and accessing data (which, however, can be used in a complementary fashion).
Consider the following:

\begin{description}
\item[Bulk Transfer] In this conceptually simple case, data transport from point A to point B is automated and augmented
with redundancy and verification mechanism so as to minimize chances of data loss. Such implementation may be needed,
for example, to transport data from the detector to the point of permanent storage.
Examples of this can be found in SPADE (data transfer system used in Daya Bay) and components of SAM at FNAL.

\item[Managed Replication] In many cases the data management strategy involves creating replicas of certain segments of the data (datasets, blocks, etc)
in participating Grid sites. Such distribution is done according to a previously developed policy which may be based on storage capacities of 
the sites, specific processing plans (cf. the concept of \textit{subscription}), resource quota and any number of other factors. Good examples of this type of systems are found in
ATLAS (\textit{Rucio)} and CMS (\textit{PhEDEx}), among other experiments~\cite{rucio_chep13,phedex_chep09}.

\item[``Data in the Grid'' (or Cloud)] In addition to accessing the data which is local to the processing element, such as a Worker Node
on the Grid, it is possible to access data that it physically remote, provided there exists enough bandwidth between the storage
facility or device, and the processing element. This can be as simple as using \textit{http} to pull data from a remote location before
executing the payload job, or as advanced as utilizing \textit{XRootD}~\cite{xrootd,xrootd_web} over WAN to federate storage resources and locate and
deliver files transparently in a ``just-in-time'' manner.

\end{description}


Choosing the right approaches and technologies in this case is obviously a two-tiered process. First, one needs to identify the most
 relevant use cases and match them to categories such as outlined above (e.g. replication vs network data on demand). Second, within
 the chosen scenario, proper solutions must be identified (and hopefully reused rather than reimplemented).
   

\subsubsection{From MONARC to a Flat Universe}
The \textbf{MONARC} architecture is a useful case study, in part because it was used in the LHC Run 1 data processing campaign,
and also because it motivated the evolution of approaches to data management which is currently under way.
It stands for \textit{Models of Networked Analysis at Regional Centers} \cite{monarc}.
At the heart of  MONARC  is a manifestly hierarchical organization of computing centers in terms of
data flow, storage and distribution policies defined based on characteristics and goals of participating sites. The sites
are classed into ``Tiers'' according to the scale of their respective resources and planned functionality, with ``Tier-0'' denomination
reserved for central facilities at CERN, ``Tier-1'' corresponding to major regional centers while ``Tier-2'' describes
sites of smaller scale, to be configured and used mainly for analysis of the data (they are also used
to handle a large fraction of the Monte Carlo simulations workload). Smaller installations and what is termed ``non-pledged resources'' 
belong to Tier-3 in this scheme, implying a more \textit{ad hoc} approach to data distribution and handling of the computational
workload on these sites. The topology of the data flow among the
Tiers can be described as somehat similar to a Directed Acyclic Graph (DAG), where data undergoes processing steps 
and is distributed from Tier-0 to a number of Tier-1 facilities, and then on to Tier-2 sites -- but Tiers of the same rank
do not share data on ``p2p'' basis.
This architecture depends on coordinated operation of two major components:
\begin{itemize}
	\item The Data Management System, that includes databases necessary to maintain records of the data parameters and location,
	and which is equipped with automated tools to move data between 
	computing centers according to chosen data processing and analysis strategies and algorithms. 
	An important component of the data handling is a subsystem for managing \textit{Metadata}, i.e. information
	derived from the actual data which is used to locate specific data segments for processing based on 
	certain selection criteria.
	
	\item The Workload Management System (WMS) -- see Section~\ref{wms} -- which distributes computational payload in accordance  
	with optimal resource availability and various applicable policies. It typically also takes
	into account data locality in order to minimize network traffic and expedite execution. A mature 
	and robust WMS also contains an efficient and user-friendly monitoring capabilities, which allows 
	its operators to monitor and troubleshoot workflows executed on the Grid.
	
\end{itemize}

While there were a variety of factors which motivated this architecture, considerations of overall efficiency, given
limits of storage capacity and network throughput, were the primary drivers in the MONARC model. In particular,
reconstruction, reprocessing and some initial stages of data reduction are typically done at the sites with
ample  storage capacity so as to avoid moving large amount of data over the network. As such, it can be argued 
that the MONARC architecture was ultimately influenced by certain assumptions about bandwidth, performance 
and reliability of networks which some authors now call ``pessimistic'' \cite [p.~105]{lhc_model_update}.

At the time when LHC computing was becoming mature, great progress was made in improving
characteristics of the networks serving the LHC projects. New generation of networks has lower
latencies, lower cost per unit of bandwidth and higher capacity. This applies to both local and wide 
area networks  \cite[p.104]{lhc_model_update}. This development opens new and significant possibilities 
which were not available until relatively recently; as stated in \cite{lhc_model_update},

\begin{quote}
The performance of the network has allowed a more flexible model in terms of data access:
	
	\begin{itemize}
		\item Removal of the strict hierarchy of data moving down the tiers, and allowing a
		more peer--peer data access policy (a site can obtain data from more or less any 
		other site);
		
		\item The introduction of the ability to have remote access to data, either in obtaining
		missing files needed by a job from over the WAN, or in some cases actually
		streaming data remotely to a job.
		
	\end{itemize}
\end{quote}

In practice, this new model results in a structure which is more ``flat'' and less hierarchical \cite{lhc_model_update}, \cite{courier_update} -- and indeed
resembles the ``p2p'' architecture.

In principle, this updated architecture does not necessarily require new networking and data transmission 
technologies when compared to MONARC, as it mainly represents a different logic and policies for 
distribution of, and access to data across multiple Grid sites. Still, there are a number of differing 
protocols and systems which are more conducive to implementing this approach than others, for a variety of reasons:

\begin{itemize}
	\item Reliance on proven, widely available and low-maintenance tools to actuate data transfer (e.g. utilizing HTTP/WebDAV).
	\item Automation of data discovery in the distributed storage.
	\item Transparent and automated "pull" of required data to local storage.
\end{itemize}

One outstanding example of leveraging the improved networking technology is \textit{XRootD}
\cite{xrootd}, \cite{xrootd_web} -- a system which facilitates \textit{federation} of widely 
distributed resources~\cite{xrootd_fed,xrootd_snowmass}. While its use in HEP is widespread, two large-scale applications deserve a special mention: 
it is employed  in the form of CMS's ``Any Data, Anytime, Anywhere'' (AAA)
project and ATLAS's ``Federating ATLAS storage systems using Xrootd" (FAX) project, both of which rely
on XRootD as their underlying technology. ``These systems are already giving experiments and
individual users greater exibility in how and where they run their workflows by making data more globally
available for access. Potential issues with bandwidth can be solved through optimization and prioritization''\cite{xrootd_snowmass}.


\subsection{Metadata}
To be able to manage the data it has to be described. Metadata, data derived from the data, is therefore a necessary part of data management. The 
list of possible metadata is long. Some key ones are:

\begin{itemize}
\item Data Provenance i.e. when and where it was taken, which raw data were used if the data are processed etc.
\item Data processing description i.e. processing status including calibrations, code versions etc.
\item Data location i.e. where the files are located - especially since data may be moved around. Note that the meaning of this changes in a 
Federated Storage Model. 
\item Data description i.e. analysis summary information.
\end{itemize}



\subsection{Data Catalogs And Logical Data Sets}
A data catalog combines a file catalog i.e. information about where the data files are stored, with additional metadata.
This enables the construction of logical (virtual) data sets like 'WIMPcandidatesLoose' and makes it possible for users to 
make a selection of a subset of the available data. 

\begin{description}
\item[Fermi Data Catalog] One way to do this is to register all the metadata when you register the file. A slightly
different approach was chosen by the Fermi Space Telescope Data Catalog. In addition to the initial metadata, it had a
data crawler that would go through all registered files and extract metadata like number of events etc. The advantage is
that the set of metadata then can be easily expanded after the fact - you just let loose the crawler with the list
of new quantities to extract which then is just added to the existing list of metadata). Obviously this only works for 
metadata included in the file and file headers.  Note that since the Fermi 
Data Catalog is independent of any workflow management system, any data processing metadata will have to be explicitly added. 



\item[SAM] .... [SAM is more tied into workflow management than the Fermi Data catalog]. Please add something.

\item[ATLAS] .... Please add something.

\item[CMS] .... Please add something.

\end{description}

\subsection{Small and Medium Scale Experiments}
Small and medium scale experiments will have more limited needs than described in the previous sections. Data will often be stored in a single or 
just a few geographical locations ('host laboratories'). Data processing is less distributed. However, most experiments today have data volumes large 
enough that they need a real data management system and not rely on a purely manually driven system (files in unix directories and wiki pages). 
And in fact, we often find many of the same elements, i.e. extensive metadata, data catalogs, XRootD, also used by smaller 
experiments. The main difference with respect to the LHC scale is the very limited technical expertise and manpower available to develop, adapt and 
operate these tools.





\subsection{Opportunities For Improvement}
One problem with Data Management systems is that they often tend to become monolithic as more and more functionality is 
added (organically). While this may make it easier to operate in the short term, it makes it more difficult to maintain over 
the long term. In particular, it makes it difficult to react to technical developments and update parts of the system. It's 
therefore critical to make the system as modular as possible and avoid tight couplings to either specific technologies or 
to other parts of the ecosystem, in particular the coupling to the Workload Management System. Modularity should therefore be 
part of the core design. This would also make reuse easier to achieve. 
\par Smaller experiments have different problems. Most small experiments have or will enter the PB era and can no onger use a manual 
data management system built on an army of grad students. They need modern data management tools. However, they have needer the 
expertise to adapt LHC scale tools for their use, neither the technical manpower to operate them. Simplifying and downscaling existing 
large scale tools to minimize necessary technical expertise and manpower to operate them, even at the price of decreasing functionality, 
may therefore be a good option. 
\par A second option is to take existing medium-scale data handling tools and repackage them for more general use. The problem is, 
however, somewhat similar to what is described above. Often these systems have become monolithic, have strong couplings to certain 
technologies and significant work may be 
necessary to make them modular. This can be difficult to achieve within the limited resources available and will need dedicated support.
\par Last, the success of Federated Storage built on XRootD shows the importance of good building blocks and how they can be arranged into 
larger successful systems.





\subsection{Common Elements vs Experiment Specific Ones}



\subsubsection{Editing notes}

\fixme{This is not a real section and will be deleted}

\begin{itemize}
\item Define data management and it's major elements (one suggestion is below)
\item reference that it is related to workflow management
\item List ways that DM can tend to be particularly parochial and how in some cases it must be specifically tailored to the computing facility and/or experiment.
\item List what elements are general.
\item Take care not to overlap with the systems group, but to point out where there are areas of overlap.
\end{itemize}
Possible partitioning of DM into smaller parts:
\begin{description}
\item[Distribution] issues:
  \begin{itemize}
  \item authentication/authorization
  \item caching / purging
  \item side hardware and software requirements
  \item on-demand vs. scheduled
  \end{itemize}
\item[Metadata] issues:
  \begin{itemize}
  \item What job produced what file and with what input parameters/data
  \item Where is my file? Data catalogs ...
  \item Fileset definitions
  \item File popularity to drive cache purges
  \item Important analysis summary info
  \item Locating files/events/objects
  \item Everything as metadata
  \item Provenance tracking
  \item Namespace issues
  \end{itemize}
\end{description}



\subsection{Description}
\subsection{What works, what doesn't}
\subsection{Examples}
\subsection{Opportunity for improvement}
